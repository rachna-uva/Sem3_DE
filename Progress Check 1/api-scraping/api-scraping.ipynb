{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edc8312",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0953c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Rachna Mallara\"\n",
    "STUDENT_ID = \"14444372\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669fd616",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf49909-80cf-409a-b44d-8553e16effc6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b585cf74037b4c712eb518f3ef8e765",
     "grade": false,
     "grade_id": "cell-3cff1dd74792b356",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Analyzing Gender Distribution Among Scientific Authors in Computational Social Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620fa176-4f89-4b75-96ba-634fc9b8962a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee897c4f558978be6819e4f4f7c261f5",
     "grade": false,
     "grade_id": "cell-8c3327acf8f9bd6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Objective*: Understand the gender distribution of authors across different scientific disciplines using web scraping and API-based gender identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39693a-b0d4-4372-a0e0-ecca61ba40c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c16f3e433bf6dac45bbe3f424b3b397",
     "grade": false,
     "grade_id": "cell-e817d3557fb4f0df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Gender diversity in research is crucial for ensuring diverse perspectives and approaches in scientific inquiry, and for the comprehensiveness and richness of research findings. A balanced gender representation can help challenge systemic biases that might otherwise marginalize or overlook significant areas of study. A diverse research community can also act as a role model, inspiring future generations of all genders to pursue scientific endeavors.\n",
    "\n",
    "This assignment focuses on the question of the gender distribution of researchers in different disciplines, and on identifying how often women are the first or last author of publications. \n",
    "\n",
    "To do so, you will scrape a preprint website, and you will use the API genderize.io to identify the gender of the author based on their name.\n",
    "\n",
    "1. Prepare: Identify a source and decide a scraping strategy\n",
    "\n",
    "2. Scrape the list of articles and authors\n",
    "\n",
    "3. Use API to identify gender \n",
    "\n",
    "4. Analyze gender distribution and authorship order\n",
    "\n",
    "5. Reflect on your findings. \n",
    "\n",
    "6. Scrape the paper abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f155b-74cb-427d-8650-9b76782acb00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1dda8234fd20fa3e245cee7db24992a9",
     "grade": false,
     "grade_id": "cell-df1834b4ac2ae69b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Setup and requirements\n",
    "First make sure that you have the needed libraries for Python correctly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6ecfdd-54b5-49ab-b84b-279c4831b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium\n",
    "# !pip install selenium\n",
    "# !pip install webdriver-manager\n",
    "# !pip install webdriver-manager --upgrade\n",
    "# !pip install packaging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "# driver.get(\"https://www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b9c67f-2327-4373-9a39-f7b94360b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request\n",
    "#!pip install requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb1d394-4f99-49d2-a25c-edf6af82561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautifulsoup\n",
    "#!pip install beautifulsoup\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6550b7-edfc-47df-af69-3d9665090102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12a9d5-8975-425f-8e13-dc4e586090ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30d4be4fd67982bfddf22fbe5c9fdf48",
     "grade": false,
     "grade_id": "cell-dcc2e17cee32d989",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Plan and strategize\n",
    "\n",
    "We first need to decide which site to scrape and our strategy for doing so. We will focus on a preprint repository. Preprint repositories host and disseminate research papers before they are peer-reviewed and published in academic journals. They therefore give a view of the latest research.\n",
    "\n",
    "There are several repositories that represent different scientific disciplines (e.g., PubMed for life sciences, arXiv for physics and computer science, JSTOR for humanities and social sciences, SocArxiv for social science, etc.) \n",
    "\n",
    "We will here focus on arxiv.org, where many Computational Social Scientists publish, often under the category \"Computers and Society\".\n",
    "\n",
    "You need to pick a page on ArXiv where you can get a representative sample of these research papers -- and which you are allowed to scrape.\n",
    "\n",
    "1. Browse Arxiv.org, and select a page on the website where you can find a sample of research papers.\n",
    "2. Check the robots.txt. Are you allowed to scrape the page you selected? (If not, you will have to choose another one!)\n",
    "3. Decide a strategy for scraping the page as quickly and easily as possible to find the names of the authors for each paper, their titles, and a link to the pages.\n",
    "4. Choose which Python libraries for scraping that you will use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723d96b-af05-42a4-981b-4d8d1722e22d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6b6e1409e86d810ef169596cbabc039",
     "grade": false,
     "grade_id": "cell-4696e716d4f1c81a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 1: Which library is most suitable?\n",
    "\n",
    "Given the structure of the website, which Python libraries for scraping do you think is appropriate to use? Motivate your choice in a few sentences.\n",
    "\n",
    "_[Student answers here.]_\n",
    "\n",
    "[Evaluation: This is an open question. Any motivation that makes sense is fine, but in general, requests make more sense for this page than selenium, since the site in question is not dynamic. Using selenium will be slower and more difficult.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4455efa-4750-4473-aa08-db19ba9bafce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "594b965b0d990ca90076303a53a26d39",
     "grade": false,
     "grade_id": "cell-f043dfd5a37ede8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Scrape the list of articles and authors \n",
    "\n",
    "Implement your scraping strategy. Scrape the page and collect the information about the publication. \n",
    "\n",
    "- You will need to get (1) the link to the article, (2) the title of the article, (3) the names of all authors of the paper, in the same order as they appear on the paper. \n",
    "- You need to scrape 200 research papers.\n",
    "\n",
    "- Note that you may need to iterate over multiple pages.\n",
    "- Note that you need to handle possible exceptions and that your code needs to be able to restart if it crashes.\n",
    "- You final result should be a list of dicts, with keys 'title', 'url', and 'authors'. 'authors' should consist of a list where the authors are listed in the order that they were on the paper. \n",
    "- You need to clean and validate your data: check that all papers have authors, that all papers have titles, clean the texts to remove empty spaces and similar, etc.\n",
    "- Store the resulting array persistently as a pickle with the name 'scraping_result.pkl'.\n",
    "\n",
    "For instance: [{'title': 'How to use Large Langauge Models for Text Analysis', 'authors': ['TÃ¶rnberg, Petter'], 'url':'https://arxiv.org/abs/2307.13106' } ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2425d0c5-0b6b-4818-945e-b9d30b4d4dc7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53252983d2c99f0b67a82089ef4b22fb",
     "grade": false,
     "grade_id": "cell-f23671663c59f383",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m\n\u001b[0;32m     36\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#print(data_list)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import copy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "data_list = []\n",
    "dict_papers = {}\n",
    "\n",
    "for papers in range (0, 200, 50):\n",
    "    #open the webpage using selenium\n",
    "    url = f'https://arxiv.org/list/cs/pastweek?skip={papers}&show=50'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    for i in range(2, 102, 2):\n",
    "        #extract title as string\n",
    "        title = soup.select(f'#dlpage > dl > dd:nth-child({i}) > div > div.list-title.mathjax')\n",
    "        title_text = title[0].get_text()[8:]\n",
    "        title_text = title_text.strip(\"\\n\")\n",
    "        dict_papers[\"title\"] = title_text\n",
    "\n",
    "        #extract author names as list of strings\n",
    "        authors = soup.select(f'#dlpage > dl > dd:nth-child({i}) > div > div.list-authors')\n",
    "        author_text = authors[0].get_text()[10:].split(\", \\n\")\n",
    "        author_text[-1] = author_text[-1].strip(\"\\n\")\n",
    "        dict_papers[\"authors\"] = author_text\n",
    "\n",
    "        #get url as string\n",
    "        url_paper = soup.select(f'#dlpage > dl > dt:nth-child({i-1}) > span > a:nth-child(1)')\n",
    "        url_text = 'https://arxiv.org' + url_paper[0].get(\"href\")\n",
    "        dict_papers[\"url\"] = url_text\n",
    "\n",
    "        data_list.append(copy.deepcopy(dict_papers))\n",
    "    \n",
    "    time.sleep(15)\n",
    "\n",
    "#print(data_list)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ad4bad-9014-462a-927e-23db9b9aa6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the list\n",
    "\n",
    "with open('scraping_result.pkl', 'wb') as f:\n",
    "    pickle.dump(data_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13393063",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a24a8ecb5c00b7a230f1536e1032db2e",
     "grade": true,
     "grade_id": "cell-eb1c8710173e94df",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check if keys exists in dictionary\n",
    "assert 'title' in data_list[0], \"Key 'title' not found in dictionary\"\n",
    "assert 'authors' in data_list[0], \"Key 'author' not found in dictionary\"\n",
    "assert 'url' in data_list[0], \"Key 'url' not found in dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac684d5-9ff9-48f2-81bf-1aa4e18ade2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb97b7c9455088e95da62d87f93f0237",
     "grade": true,
     "grade_id": "cell-a20385a62f48c168",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d989cacd-dff0-456e-8967-2dbe08362fc0",
   "metadata": {},
   "source": [
    "## 3. Use Genderize.io to identify author gender\n",
    "\n",
    "The next step is to identify the gender of the authors. To do so, we will use the free API genderdize.io. \n",
    "\n",
    "1. Go to https://genderize.io/ and read the API documnentation.\n",
    "2. Do you need to register to use it? Do you need an API key? \n",
    "3. How do you call the API? What parameters do you need to send? \n",
    "4. What rate limiting is used? How long do you need to wait between calls?\n",
    "\n",
    "You will use what you learned to carry out the following tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065292a-42e8-4069-8dfd-4d6a1db353fa",
   "metadata": {},
   "source": [
    "#### Task 1: _identify_gender()_\n",
    "Write a function _identify_gender(first_name)_ that takes a name, and uses the API to guess the gender. The function should send a request to genderize.io, and parse the resulting json to a dict. The function should return a dict with the data provided by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d01c681e-6bce-416d-9c45-03810e9c2376",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8699ffe64f521fe51e152a3c4baae18",
     "grade": false,
     "grade_id": "cell-c02e3ed6f4cf8078",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'first_name': 'None', 'estimated_gender': nan, 'gender_probability': nan}, {'first_name': 'Xi', 'estimated_gender': 'female', 'gender_probability': 0.97}]\n"
     ]
    }
   ],
   "source": [
    "#Written with help from code in this Github repository: https://github.com/acceptable-security/gender.py\n",
    "\n",
    "def identify_gender(*args):\n",
    "    base_url = \"https://api.genderize.io/?name[]=\"\n",
    "\n",
    "    list_names = list(args)\n",
    "\n",
    "    # Batch the names into groups of 10 or less\n",
    "    batches = [list_names[i:i + 10] for i in range(0, len(list_names), 10)]\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for batch in batches:\n",
    "        url = base_url + '&name[]='.join(batch)\n",
    "        \n",
    "        req = requests.get(url)\n",
    "        results = json.loads(req.text)\n",
    "        dict_name = {}\n",
    "\n",
    "        for result in results:\n",
    "            if 'error' not in result:\n",
    "                if result[\"gender\"] is not None:\n",
    "                    dict_name[\"first_name\"] = result[\"name\"]\n",
    "                    dict_name[\"estimated_gender\"] = result[\"gender\"]\n",
    "                    dict_name[\"gender_probability\"] = result[\"probability\"]\n",
    "                else:\n",
    "                    dict_name[\"first_name\"] = \"None\"\n",
    "                    dict_name[\"estimated_gender\"] = np.nan\n",
    "                    dict_name[\"gender_probability\"] = np.nan\n",
    "            else:\n",
    "                dict_name[\"first_name\"] = \"None\"\n",
    "                dict_name[\"estimated_gender\"] = np.nan\n",
    "                dict_name[\"gender_probability\"] = np.nan\n",
    "            \n",
    "            results_list.append(copy.deepcopy(dict_name))\n",
    "\n",
    "    return results_list\n",
    "\n",
    "# Example function call\n",
    "names = [\"Owen\", \"Martha\", \"Google\", \"Ram\", \"Sushmita\", \"Karen\", \"Vivek\", \"Pulkit\", \"Hannah\", \"Rachna\", \"Xi\"]\n",
    "print(identify_gender(*names))\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc4dbd-cecb-45de-9cc0-067c83acb78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7dc7f-c47f-4fb1-85db-096a46d5d5c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05b69c0a6698acd0a17d3faae33a79f8",
     "grade": true,
     "grade_id": "cell-ed3a813912da7ef2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d03ef969-2f1e-4ebf-bd1f-3dd0657f2e69",
   "metadata": {},
   "source": [
    "#### Task 2: Identify gender of all authors\n",
    "\n",
    "Your task is now to use your new function to identify the genders of all authors that you previously scraped. \n",
    "\n",
    "To do so, you first need to extract the first name of each author. You need to iterate over these names and use your function to identify the gender of the author.\n",
    "\n",
    "Your result should be a dataframe with the following columns:\n",
    "\n",
    "- article_url | author_full_name | first_name | author_order | estimated_gender | gender_probability\n",
    "\n",
    "Author_order should be a number specifying where the author was in the author list for the publication (e.g., 0 = first author, 1 = second author...) _Estimated_gender_ should contain the API response on gender, and _gender_probability_ the certainty of the gender, according to the API.\n",
    "\n",
    "Note:\n",
    "- You will need to transform your dict to the dataframe shown above, with one author per line. (This means that each URL will be associated to multiple author names.)\n",
    "- Make sure that you respect the rate limiting of the API. \n",
    "- Make sure that you handle exceptions and that your function continues \n",
    "- Note that you get a maximum of 1,000 free calls per day, so you need to make sure that you do not waste your API calls!\n",
    "- The API may not have all names stored. For these names, store a _np.nan_ value as the gender.\n",
    "\n",
    "Pickle the resulting dataframe with the name: 'author_gender.df.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1db8df5b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a2558664a48ddbac7c92c35c0c14e2b",
     "grade": false,
     "grade_id": "cell-b9581c1efe1807cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def name_splitter(writers):\n",
    "    first_names = []\n",
    "    \n",
    "    for full_name in writers:\n",
    "        first_name = full_name.split()[0]\n",
    "        if '-' in first_name:\n",
    "            first_name = first_name[0 : first_name.index('-')]\n",
    "        first_names.append(first_name)\n",
    "    \n",
    "    return first_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1287db8b-4a3a-4ab4-a242-96dbcaf651c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        article_url      author_full_name first_name  \\\n",
      "0  https://arxiv.org/abs/2311.10709         Rohit Girdhar       None   \n",
      "1  https://arxiv.org/abs/2311.10708  Sai Saketh Rambhatla       None   \n",
      "2  https://arxiv.org/abs/2311.10707         Xiaohui Zhang       None   \n",
      "3  https://arxiv.org/abs/2311.10706          Zhongtian He       None   \n",
      "4  https://arxiv.org/abs/2311.10702         Hamish Ivison       None   \n",
      "\n",
      "   author_order  estimated_gender  gender_probability  \n",
      "0             1               NaN                 NaN  \n",
      "1             1               NaN                 NaN  \n",
      "2             1               NaN                 NaN  \n",
      "3             1               NaN                 NaN  \n",
      "4             1               NaN                 NaN  \n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "# Load scraped data\n",
    "with open('scraping_result.pkl', 'rb') as f:\n",
    "    data_list = pickle.load(f)\n",
    "\n",
    "authors = []\n",
    "papers_df = [] #will be converted to df later\n",
    "\n",
    "for record in data_list:\n",
    "    authors = record[\"authors\"]   #find list of authors\n",
    "    #print(authors)\n",
    "    first_names = name_splitter(authors) #obtain list of first names\n",
    "    #print(first_names)\n",
    "    results = identify_gender(*first_names) #obtain list of dicts with name and gender details\n",
    "   \n",
    "    for i, result in enumerate(results):\n",
    "        papers_df.append([record[\"url\"], authors[i], result[\"first_name\"], i+1, result[\"estimated_gender\"], result[\"gender_probability\"]])\n",
    "\n",
    "# convert to dataframe\n",
    "papers_df = pd.DataFrame(papers_df, columns=['article_url', 'author_full_name', 'first_name', 'author_order', 'estimated_gender', 'gender_probability'])\n",
    "\n",
    "print(papers_df.head())\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa0715-07c8-46a0-8088-516ae35837b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002fc50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94b2428a7e2b93e5c2e81bca2122b796",
     "grade": true,
     "grade_id": "cell-85d226433e71226b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'article_url' in df.columns, \"article_url column is missing\"\n",
    "assert 'author_full_name' in df.columns, \"author_full_name column is missing\"\n",
    "assert 'first_name' in df.columns, \"first_name column is missing\"\n",
    "assert 'author_order' in df.columns, \"author_order column is missing\"\n",
    "assert 'estimated_gender' in df.columns, \"estimated_gender column is missing\"\n",
    "assert 'gender_probability' in df.columns, \"gender_probability column is missing\"\n",
    "\n",
    "with open('author_gender.df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04701544-19ce-4f09-b6fc-06d1cae74f58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f43b38f6f15f75ac827995ace4fe885",
     "grade": true,
     "grade_id": "cell-c2e95f57fe249bfe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efaadaf-c7cf-4287-88a4-27b2bdf846e4",
   "metadata": {},
   "source": [
    "## 4. Analyze gender distribution and authorship order\n",
    "\n",
    "Now that you have gathered the necessary data, you will use this data to answer some research questions about gender equality in CSS research. Note that in calculating this, you need to handle that the API may have failed to identify the gender of some authors.\n",
    "\n",
    "1. What fraction of the authors included are women? \n",
    "2. What happens to this fraction if you only include authors for which the gender_probability is higher than 80%? \n",
    "3. Being the first or single author on a research paper is an important status signal among researchers: it often means that you made the most work. What fraction of the first or single authors are women? \n",
    "4. Being the _last_ author on a research paper with _three or more authors_ is also an important status signal: it tends to mean that you were the one to acquire funding or lead the lab. What fraction of the last-authors on papers with three or more author are women?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbbcb4f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e46a8a2f7e1cabdb0061c0d4c9fc86f2",
     "grade": true,
     "grade_id": "cell-3a0cd012a322701a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b370f3-4b58-41f0-b615-5aff1c4a3ab6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66aa822667d179fcd1119f84e5100ee3",
     "grade": false,
     "grade_id": "cell-906708bcf0e226e3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## 5. Reflect on your findings\n",
    "\n",
    "You have now carried out your analysis of the gender distribution in articles in CSS using scraped data. Reflect on your findings and method, and answer each of the following questions in a few sentences.\n",
    "\n",
    "1. What are the implications of the observed gender distribution and author order in CSS? How do these distributions compare with your expectations?\n",
    "2. How accurate do you think your findings are? What are the limitations of determining gender based solely on names? Are there cultural or regional nuances that the API might miss?\n",
    "3. Reflect on the ethical considerations involved in scraping this data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27044038-f459-45c7-9a7f-d2cd6d15b18d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "253f673902e8de295d8442ad59caa4b8",
     "grade": true,
     "grade_id": "cell-47e469c422287e72",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d395f6-7186-4239-93db-e3aa75f6abfb",
   "metadata": {},
   "source": [
    "## 6. Scrape the paper abstract\n",
    "\n",
    "Your next task is to get the abstract for each paper. You will use these abstracts in a later exercise in the course, where we will use text analysis to examine whether the content of research papers are a function of the gender of the author. \n",
    "\n",
    "To do so, you need to iterate over the papers that you have already identified, and scrape the abstract from the URL listed. \n",
    "\n",
    "#### Task 1: scrape_abstract()\n",
    "Write a function scrape_abstract(url) that goes to the research paper URL, and scrapes the content of the abstract. The function should return the abstract as a string, and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98b5f1-ffd3-4150-a60c-2e738c884956",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea05833922353eeb9ca3fd40d86801b7",
     "grade": false,
     "grade_id": "cell-b8712361e6327b18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_abstract(url):\n",
    "    \"\"\"\n",
    "    Fetch the abstract from the provided arXiv URL using XPath.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the arXiv paper.\n",
    "\n",
    "    Returns:\n",
    "    - str: The abstract of the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Test\n",
    "url = \"https://arxiv.org/abs/2307.13106\"\n",
    "print(scrape_abstract(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4466e2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "284598e8243153347beb6d3b0f5691f7",
     "grade": true,
     "grade_id": "cell-862386d3cf70edf2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4783d9a5-b38d-4ccc-9cbe-f2269467e6d8",
   "metadata": {},
   "source": [
    "#### Task 2: Scrape all urls\n",
    "\n",
    "You will now use your function to scrape all the URLs that you collected in step 2.\n",
    "\n",
    "The following will provide instructions for how you can go about this task. However, there are several ways to do this, and you are free to choose your preferred method.\n",
    "\n",
    "Prepare your data:\n",
    "\n",
    "1. Load your list of dicts from step 2 (scraping_result.pkl)\n",
    "2. Use it to create a dataframe. \n",
    "3. Add a column 'scraped' which should be False for all rows, and a column 'abstract' that should be None for all rows.\n",
    "4. Store the dataframe persistently (e.g., by pickling it.)\n",
    "\n",
    "The scraping procedure:\n",
    "\n",
    "1. Load the persitent pickle as dataframe (so that if your computer crashes, the function will continue where you were)\n",
    "2. Repeat the following steps until there are no more rows with scraped == False:\n",
    "3. Fetch a random row with scraped == False\n",
    "4. Go to the URL and scrape the abstract.\n",
    "5. Set abstract column in the dataframe to the resulting abstract, set scraped to True.\n",
    "6. Store the dataframe persistently as a pickle. \n",
    "\n",
    "Remember: \n",
    "- You may use another strategy. However, since you will be scraping many pages, you should expect your scraper to encounter problems along the way. You therefore need to make sure that you regularly store the results persistently.\n",
    "- Make sure to handle any exceptions gracefully.\n",
    "- Be respectful toward the website owners: wait at least one second between each call. \n",
    "\n",
    "Your final result should be a dataframe stored as 'scraped_abstracts.df.pkl', with filled 'abstract' and 'scraped' columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbacc7-e0a1-4f98-8234-0ef5f76f3489",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<!-- [Evaluation: ]\n",
    "- Load dataframe as df\n",
    "- Check that the len of df = len of the result list from question 2. \n",
    "- Check that each line has an abstract, with len() > 100 e.g.\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8f157-0b13-4a9b-b4ff-56c738754500",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc342ab1d66dd404139011d72ebc4627",
     "grade": false,
     "grade_id": "cell-ac850524f08140c8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "# Rename and save the final dataframe\n",
    "df.to_pickle('scraped_abstracts.df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95ec0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd9348c76775a31d0bc7c64630beda0c",
     "grade": true,
     "grade_id": "cell-d2927458cb5edcd5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
