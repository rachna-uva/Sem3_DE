{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f02b405",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ca3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Rachna Mallara\"\n",
    "STUDENT_ID = \"14444372\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292538f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338a427",
   "metadata": {},
   "source": [
    "*Objective*: Apply topic modelling techniques, such as Latent Dirichlet Allocation (LDA), to analyze and interpret the primary topics present in a collection of online news articles.\n",
    "\n",
    "Topic modelling is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. It is a frequently used text-mining tool for the discovery of hidden semantic structures in a text body. This assignment involves implementing and interpreting LDA topic modelling on a dataset of online news articles to understand the prevalent themes and topics.\n",
    "\n",
    "For this task, you will use the \"Fake news\" dataset, which contains information about a large number of fake news articles. The dataset is available here: https://www.kaggle.com/datasets/mrisdal/fake-news.\n",
    "\n",
    "1. Prepare: Explore the dataset\n",
    "2. Pre-process the text data\n",
    "3. Implement the LDA model\n",
    "4. Analyze the topics and interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925637f",
   "metadata": {},
   "source": [
    "### Setup and requirements\n",
    "First, make sure that you have the needed libraries for Python correctly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760b4b5a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ede09c3d5cc5dd7a861761fe3969ce2",
     "grade": false,
     "grade_id": "cell-dd19779ac384bb27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rachn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rachn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install numpy pandas matplotlib sklearn gensim nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec19996",
   "metadata": {},
   "source": [
    "## 1. Prepare and Explore the Dataset (1 point)\n",
    "\n",
    "The first step is to download and load the dataset. Familiarize yourself with its structure and content. Understand the kind of articles included, and how the data is organized.\n",
    "\n",
    "\n",
    "1. Load the dataset using pandas.\n",
    "2. Explore the dataset. What columns does it include? How are the articles represented?\n",
    "3. For exploration purposes and initial model training take 15-35% sample of dataframe using the sample method in pandas\n",
    "4. Store your dataset in the variable named `news_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd5fba8d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c08ae297a579d812729903d4d5972bb7",
     "grade": false,
     "grade_id": "cell-98f59e03d8e80d78",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The head of the original dataset fake.csv:                                        uuid  ord_in_thread  \\\n",
      "0  6a175f46bcd24d39b3e962ad0f29936721db70db              0   \n",
      "1  2bdc29d12605ef9cf3f09f9875040a7113be5d5b              0   \n",
      "2  c70e149fdd53de5e61c29281100b9de0ed268bc3              0   \n",
      "3  7cf7c15731ac2a116dd7f629bd57ea468ed70284              0   \n",
      "4  0206b54719c7e241ffe0ad4315b808290dbe6c0f              0   \n",
      "\n",
      "                 author                      published  \\\n",
      "0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n",
      "1  reasoning with facts  2016-10-29T08:47:11.259+03:00   \n",
      "2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n",
      "3                Fed Up  2016-11-01T05:22:00.000+02:00   \n",
      "4                Fed Up  2016-11-01T21:56:00.000+02:00   \n",
      "\n",
      "                                               title  \\\n",
      "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
      "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
      "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
      "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
      "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
      "\n",
      "                                                text language  \\\n",
      "0  Print They should pay all the back all the mon...  english   \n",
      "1  Why Did Attorney General Loretta Lynch Plead T...  english   \n",
      "2  Red State : \\nFox News Sunday reported this mo...  english   \n",
      "3  Email Kayla Mueller was a prisoner and torture...  english   \n",
      "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  english   \n",
      "\n",
      "                         crawled             site_url country  domain_rank  \\\n",
      "0  2016-10-27T01:49:27.168+03:00  100percentfedup.com      US      25689.0   \n",
      "1  2016-10-29T08:47:11.259+03:00  100percentfedup.com      US      25689.0   \n",
      "2  2016-10-31T01:41:49.479+02:00  100percentfedup.com      US      25689.0   \n",
      "3  2016-11-01T15:46:26.304+02:00  100percentfedup.com      US      25689.0   \n",
      "4  2016-11-01T23:59:42.266+02:00  100percentfedup.com      US      25689.0   \n",
      "\n",
      "                                        thread_title  spam_score  \\\n",
      "0  Muslims BUSTED: They Stole Millions In Gov’t B...       0.000   \n",
      "1  Re: Why Did Attorney General Loretta Lynch Ple...       0.000   \n",
      "2  BREAKING: Weiner Cooperating With FBI On Hilla...       0.000   \n",
      "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...       0.068   \n",
      "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...       0.865   \n",
      "\n",
      "                                        main_img_url  replies_count  \\\n",
      "0  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
      "1  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
      "2  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
      "3  http://100percentfedup.com/wp-content/uploads/...              0   \n",
      "4  http://100percentfedup.com/wp-content/uploads/...              0   \n",
      "\n",
      "   participants_count  likes  comments  shares  type  \n",
      "0                   1      0         0       0  bias  \n",
      "1                   1      0         0       0  bias  \n",
      "2                   1      0         0       0  bias  \n",
      "3                   0      0         0       0  bias  \n",
      "4                   0      0         0       0  bias  .\n",
      "The columns in the dataset are: Index(['uuid', 'ord_in_thread', 'author', 'published', 'title', 'text',\n",
      "       'language', 'crawled', 'site_url', 'country', 'domain_rank',\n",
      "       'thread_title', 'spam_score', 'main_img_url', 'replies_count',\n",
      "       'participants_count', 'likes', 'comments', 'shares', 'type'],\n",
      "      dtype='object').\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_df = pd.read_csv('fake.csv')\n",
    "print(f'The head of the original dataset fake.csv: {news_df.head()}.')\n",
    "print(f'The columns in the dataset are: {news_df.columns}.')\n",
    "\n",
    "news_df = news_df.sample(frac = 0.2, random_state = 5)\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669b7c40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66e5ae238c748c57795da1f5433edb7d",
     "grade": true,
     "grade_id": "cell-b34716260ad1c575",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 1949 <= len(news_df) <= 4550, \"You should sample between 15-35% of the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d06419",
   "metadata": {},
   "source": [
    "### Question 1: Dataset Exploration (1 point)\n",
    "\n",
    "\n",
    "What are the key characteristics of this dataset? Describe the dataset in terms of its size, variety of articles, and any other notable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "706024b1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9575b390b4cc35d1a33a152c7056b8b",
     "grade": true,
     "grade_id": "cell-a06327cbfa6e1933",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about sampled dataset: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2600 entries, 1680 to 11756\n",
      "Data columns (total 20 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   uuid                2600 non-null   object \n",
      " 1   ord_in_thread       2600 non-null   int64  \n",
      " 2   author              2131 non-null   object \n",
      " 3   published           2600 non-null   object \n",
      " 4   title               2469 non-null   object \n",
      " 5   text                2585 non-null   object \n",
      " 6   language            2600 non-null   object \n",
      " 7   crawled             2600 non-null   object \n",
      " 8   site_url            2600 non-null   object \n",
      " 9   country             2570 non-null   object \n",
      " 10  domain_rank         1772 non-null   float64\n",
      " 11  thread_title        2598 non-null   object \n",
      " 12  spam_score          2600 non-null   float64\n",
      " 13  main_img_url        1866 non-null   object \n",
      " 14  replies_count       2600 non-null   int64  \n",
      " 15  participants_count  2600 non-null   int64  \n",
      " 16  likes               2600 non-null   int64  \n",
      " 17  comments            2600 non-null   int64  \n",
      " 18  shares              2600 non-null   int64  \n",
      " 19  type                2600 non-null   object \n",
      "dtypes: float64(2), int64(6), object(12)\n",
      "memory usage: 426.6+ KB\n",
      "None\n",
      "\n",
      "Descriptive statistics: \n",
      "\n",
      "       ord_in_thread   domain_rank   spam_score  replies_count  \\\n",
      "count    2600.000000   1772.000000  2600.000000    2600.000000   \n",
      "mean        0.829615  39682.668172     0.028493       1.333077   \n",
      "std         6.129911  27203.885037     0.131364       9.998375   \n",
      "min         0.000000    486.000000     0.000000       0.000000   \n",
      "25%         0.000000  17592.000000     0.000000       0.000000   \n",
      "50%         0.000000  35381.000000     0.000000       0.000000   \n",
      "75%         0.000000  61606.000000     0.000000       0.000000   \n",
      "max        96.000000  98679.000000     1.000000     308.000000   \n",
      "\n",
      "       participants_count        likes     comments       shares  \n",
      "count         2600.000000  2600.000000  2600.000000  2600.000000  \n",
      "mean             1.716154    11.103462     0.047692    11.103462  \n",
      "std              7.350054    79.493949     0.800175    79.493949  \n",
      "min              0.000000     0.000000     0.000000     0.000000  \n",
      "25%              1.000000     0.000000     0.000000     0.000000  \n",
      "50%              1.000000     0.000000     0.000000     0.000000  \n",
      "75%              1.000000     0.000000     0.000000     0.000000  \n",
      "max            239.000000   988.000000    30.000000   988.000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>ord_in_thread</th>\n",
       "      <th>author</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>crawled</th>\n",
       "      <th>site_url</th>\n",
       "      <th>country</th>\n",
       "      <th>domain_rank</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>spam_score</th>\n",
       "      <th>main_img_url</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>participants_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>shares</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>570fc81a9e94108e68ad5b8b588a775de05f6b1c</td>\n",
       "      <td>0</td>\n",
       "      <td>Arjun Walia</td>\n",
       "      <td>2016-10-28T22:30:39.914+03:00</td>\n",
       "      <td>Breaking: FBI Reopens Investigation Into Hilla...</td>\n",
       "      <td>The FBI recently decided to reopen their inves...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-28T22:30:39.914+03:00</td>\n",
       "      <td>collective-evolution.com</td>\n",
       "      <td>US</td>\n",
       "      <td>10352.0</td>\n",
       "      <td>Breaking: FBI Reopens Investigation Into Hilla...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6785</th>\n",
       "      <td>0f5bba21671d935e445421386a9a134bf7ff9869</td>\n",
       "      <td>0</td>\n",
       "      <td>Geoffrey Grider</td>\n",
       "      <td>2016-11-24T04:10:00.000+02:00</td>\n",
       "      <td>Mayhem Awaits As Crooked Hillary Clinton Is No...</td>\n",
       "      <td>Mayhem Awaits As Crooked Hillary Clinton Is ...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-11-24T19:55:05.771+02:00</td>\n",
       "      <td>nowtheendbegins.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27046.0</td>\n",
       "      <td>Mayhem Awaits As Crooked Hillary Clinton Is No...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://nteb-mudflowermedia.netdna-ssl.com/wp-...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>c070dfd7933262cd3734f153183098a7c0b9bfc1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-11-01T08:22:00.000+02:00</td>\n",
       "      <td>SETI officially announce Extraterrestrial signals</td>\n",
       "      <td>0 \\nSOURCES AND MIRRORS CAN BE FOUND IN THE YO...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-11-01T13:28:23.438+02:00</td>\n",
       "      <td>disclose.tv</td>\n",
       "      <td>TV</td>\n",
       "      <td>25709.0</td>\n",
       "      <td>SETI officially announce Extraterrestrial signals</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://d38zt8ehae1tnt.cloudfront.net/seti_offi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>4b8d4966d3259d3abbfa4555d7a544ef60ac49e9</td>\n",
       "      <td>0</td>\n",
       "      <td>Sausage Machine</td>\n",
       "      <td>2016-11-02T21:20:50.822+02:00</td>\n",
       "      <td>The final line in this story about Liam Gallag...</td>\n",
       "      <td>The final line in this story about Liam Gallag...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-11-02T21:20:50.822+02:00</td>\n",
       "      <td>thepoke.co.uk</td>\n",
       "      <td>GB</td>\n",
       "      <td>19375.0</td>\n",
       "      <td>The final line in this story about Liam Gallag...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://www.thepoke.co.uk/wp-content/uploads/20...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6007</th>\n",
       "      <td>6ca50fce5fcee628b8b01da924015c7a658a758d</td>\n",
       "      <td>0</td>\n",
       "      <td>Prissy Holly</td>\n",
       "      <td>2016-10-27T21:12:27.421+03:00</td>\n",
       "      <td>BUSTED: Clinton Foundation Directly Tied To Pl...</td>\n",
       "      <td>BUSTED: Clinton Foundation Directly Tied To Pl...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-27T21:12:27.421+03:00</td>\n",
       "      <td>madworldnews.com</td>\n",
       "      <td>US</td>\n",
       "      <td>23040.0</td>\n",
       "      <td>BUSTED: Clinton Foundation Directly Tied To Pl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://madworldnews.com/wp-content/uploads/201...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           uuid  ord_in_thread  \\\n",
       "1680   570fc81a9e94108e68ad5b8b588a775de05f6b1c              0   \n",
       "6785   0f5bba21671d935e445421386a9a134bf7ff9869              0   \n",
       "2745   c070dfd7933262cd3734f153183098a7c0b9bfc1              0   \n",
       "10197  4b8d4966d3259d3abbfa4555d7a544ef60ac49e9              0   \n",
       "6007   6ca50fce5fcee628b8b01da924015c7a658a758d              0   \n",
       "\n",
       "                author                      published  \\\n",
       "1680       Arjun Walia  2016-10-28T22:30:39.914+03:00   \n",
       "6785   Geoffrey Grider  2016-11-24T04:10:00.000+02:00   \n",
       "2745               NaN  2016-11-01T08:22:00.000+02:00   \n",
       "10197  Sausage Machine  2016-11-02T21:20:50.822+02:00   \n",
       "6007      Prissy Holly  2016-10-27T21:12:27.421+03:00   \n",
       "\n",
       "                                                   title  \\\n",
       "1680   Breaking: FBI Reopens Investigation Into Hilla...   \n",
       "6785   Mayhem Awaits As Crooked Hillary Clinton Is No...   \n",
       "2745   SETI officially announce Extraterrestrial signals   \n",
       "10197  The final line in this story about Liam Gallag...   \n",
       "6007   BUSTED: Clinton Foundation Directly Tied To Pl...   \n",
       "\n",
       "                                                    text language  \\\n",
       "1680   The FBI recently decided to reopen their inves...  english   \n",
       "6785     Mayhem Awaits As Crooked Hillary Clinton Is ...  english   \n",
       "2745   0 \\nSOURCES AND MIRRORS CAN BE FOUND IN THE YO...  english   \n",
       "10197  The final line in this story about Liam Gallag...  english   \n",
       "6007   BUSTED: Clinton Foundation Directly Tied To Pl...  english   \n",
       "\n",
       "                             crawled                  site_url country  \\\n",
       "1680   2016-10-28T22:30:39.914+03:00  collective-evolution.com      US   \n",
       "6785   2016-11-24T19:55:05.771+02:00       nowtheendbegins.com     NaN   \n",
       "2745   2016-11-01T13:28:23.438+02:00               disclose.tv      TV   \n",
       "10197  2016-11-02T21:20:50.822+02:00             thepoke.co.uk      GB   \n",
       "6007   2016-10-27T21:12:27.421+03:00          madworldnews.com      US   \n",
       "\n",
       "       domain_rank                                       thread_title  \\\n",
       "1680       10352.0  Breaking: FBI Reopens Investigation Into Hilla...   \n",
       "6785       27046.0  Mayhem Awaits As Crooked Hillary Clinton Is No...   \n",
       "2745       25709.0  SETI officially announce Extraterrestrial signals   \n",
       "10197      19375.0  The final line in this story about Liam Gallag...   \n",
       "6007       23040.0  BUSTED: Clinton Foundation Directly Tied To Pl...   \n",
       "\n",
       "       spam_score                                       main_img_url  \\\n",
       "1680          0.0                                                NaN   \n",
       "6785          0.0  https://nteb-mudflowermedia.netdna-ssl.com/wp-...   \n",
       "2745          0.0  http://d38zt8ehae1tnt.cloudfront.net/seti_offi...   \n",
       "10197         0.0  http://www.thepoke.co.uk/wp-content/uploads/20...   \n",
       "6007          0.0  http://madworldnews.com/wp-content/uploads/201...   \n",
       "\n",
       "       replies_count  participants_count  likes  comments  shares type  \n",
       "1680               0                   1      0         0       0   bs  \n",
       "6785               0                   1      0         0       0   bs  \n",
       "2745               0                   0      0         0       0   bs  \n",
       "10197              0                   1      0         0       0   bs  \n",
       "6007               0                   1      0         0       0   bs  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print('Information about sampled dataset: \\n')\n",
    "print(news_df.info())\n",
    "print('\\nDescriptive statistics: \\n')\n",
    "print(news_df.describe())\n",
    "news_df.head()\n",
    "\n",
    "#what are the number of articles? what are the key data sources? what are the avg number of words per article? how many sources are there?\n",
    "\n",
    "#find the main language for the comments\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55315fc0",
   "metadata": {},
   "source": [
    "## 2. Pre-process the Text Data\n",
    "\n",
    "Before applying topic modelling, it's crucial to pre-process the text data. This involves cleaning the text, removing stop words, and converting the text into a suitable format for analysis.\n",
    "\n",
    "1. Complete the `preprocess_text()` function to clean the text data (remove punctuation, lowercase, tokenize, lemmatize).\n",
    "2. Remove stopwords using the NLTK library.\n",
    "3. Create a corpus required for the LDA model using the gensim package and save it in variable `corpus`.\n",
    "3. Convert the cleaned text into a document-term matrix using the gensim package and save it in variable `doc_term_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5790c923",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "553359e51d2d24c62caad51bd362dd2d",
     "grade": false,
     "grade_id": "cell-452f92f52df1cee7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rachn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 1: Text pre-processing function\n",
    "def preprocess_text(text):\n",
    "    # YOUR CODE HERE\n",
    "    # Handle NaN values\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Apply text pre-processing to the 'text' column\n",
    "news_df['processed_text'] = news_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Create a corpus for the LDA model\n",
    "dictionary = corpora.Dictionary(news_df['processed_text'])\n",
    "corpus = [dictionary.doc2bow(text) for text in news_df['processed_text']]\n",
    "\n",
    "# Step 3: Create a document-term matrix\n",
    "tfidf = TfidfModel(corpus)\n",
    "doc_term_matrix = [tfidf[doc] for doc in corpus]\n",
    "corpus = dictionary # for the public test lol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae420a3",
   "metadata": {},
   "source": [
    "Public test (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bec4579",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61584de91cadb7e4ec018fe1b030d6fd",
     "grade": true,
     "grade_id": "cell-7a6a25d384681e03",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(doc_term_matrix) == list, \"doc_term_matrix should be a list of lists\"\n",
    "assert type(corpus) == gensim.corpora.dictionary.Dictionary, \"corpus should be a gensim.corpora.dictionary.Dictionary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b85f0",
   "metadata": {},
   "source": [
    "Hidden tests (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f65cf6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "073ac1768af564b2deb85b71fc6c0983",
     "grade": true,
     "grade_id": "cell-becc487e03b5adcd",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5094568c",
   "metadata": {},
   "source": [
    "### Question 2: Pre-processing Importance (2 points)\n",
    "\n",
    "Why is pre-processing important in topic modelling? Describe how each step in the pre-processing pipeline contributes to the overall analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83394648",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a763c1df97ef58ae88b83a606ae2aed",
     "grade": true,
     "grade_id": "cell-f075d1d9356f0fca",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7647e",
   "metadata": {},
   "source": [
    "## 3. Implement the LDA Model (1 point)\n",
    "\n",
    "Now, it's time to implement the LDA model using the Gensim library. Be sure to check out the documentation for hyperparameter settings.\n",
    "\n",
    "1. Choose the number of topics for the model. This is a crucial step and may require some experimentation.\n",
    "2. Train the LDA model on the dataset.\n",
    "3. Save the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27446851",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42d5c65fabe90db6029d1e7cffeecd42",
     "grade": false,
     "grade_id": "cell-5dcaf729b499fc30",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# evaluate model to see if num_topics_selected should be modified - highest coherence was for num_topics_selected = 2\n",
    "\"\"\"\n",
    "for num_topics_selected in range(1, 11):\n",
    "    lda_model = LdaModel(corpus = doc_term_matrix, num_topics = num_topics_selected, id2word = corpus)\n",
    "    coherence_model = CoherenceModel(model = lda_model, texts = news_df['processed_text'], dictionary = corpus, coherence = 'c_v')\n",
    "    print(f'Num Topics: {num_topics_selected}, Coherence Score: {coherence_model.get_coherence()}')\n",
    "\"\"\"\n",
    "# generate lda model\n",
    "num_topics_selected = 5\n",
    "lda_model = LdaModel(corpus = doc_term_matrix, num_topics = num_topics_selected, id2word = corpus)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da73eb16",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3be4204c1ea48dfdf3a19157a08669a",
     "grade": true,
     "grade_id": "cell-d6e66bfedef344aa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(lda_model) == gensim.models.ldamodel.LdaModel, \"lda_model should be a gensim.models.ldamodel.LdaModel\"\n",
    "lda_model.save('lda_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0122d3e6",
   "metadata": {},
   "source": [
    "### Question 3: Model Parameters (2 points)\n",
    "\n",
    "Discuss the choice of number of topics for the LDA model. How does this choice impact the model's performance and the interpretability of the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca03053",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25e479cecd029ff3c4976c7de22369f8",
     "grade": true,
     "grade_id": "cell-7ac44640f28f4dc1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e01ea5",
   "metadata": {},
   "source": [
    "## 4. Analyze Topics and Interpret Results (1 point)\n",
    "\n",
    "Finally, analyze the topics produced by the LDA model and interpret the results.\n",
    "\n",
    "1. Use the LDA model to identify the main topics in the dataset.\n",
    "2. For each topic, examine the most representative words.\n",
    "4. Interpret the topics: What themes or subjects do they represent?\n",
    "\n",
    "### Question 4: Topic Interpretation\n",
    "\n",
    "Interpret the topics generated by the LDA model. How coherent are the topics? What do they tell us about the content of the dataset? Does this model need improvement by modifying parameters, using further pre-processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee3983b1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6221f043a19d983619a2037ac8ab415f",
     "grade": true,
     "grade_id": "cell-b2dff7e06fc82bd6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "trump (Score: 0.0011)\n",
      "clinton (Score: 0.0008)\n",
      "election (Score: 0.0006)\n",
      "hillary (Score: 0.0006)\n",
      "people (Score: 0.0005)\n",
      "u (Score: 0.0005)\n",
      "vote (Score: 0.0005)\n",
      "war (Score: 0.0005)\n",
      "know (Score: 0.0005)\n",
      "obama (Score: 0.0005)\n",
      "russia (Score: 0.0005)\n",
      "campaign (Score: 0.0004)\n",
      "would (Score: 0.0004)\n",
      "email (Score: 0.0004)\n",
      "woman (Score: 0.0004)\n",
      "state (Score: 0.0004)\n",
      "one (Score: 0.0004)\n",
      "president (Score: 0.0004)\n",
      "like (Score: 0.0004)\n",
      "party (Score: 0.0004)\n",
      "time (Score: 0.0004)\n",
      "dont (Score: 0.0004)\n",
      "american (Score: 0.0004)\n",
      "think (Score: 0.0004)\n",
      "said (Score: 0.0004)\n",
      "world (Score: 0.0004)\n",
      "medium (Score: 0.0003)\n",
      "life (Score: 0.0003)\n",
      "donald (Score: 0.0003)\n",
      "country (Score: 0.0003)\n",
      "\n",
      "Topic 2:\n",
      "trump (Score: 0.0010)\n",
      "clinton (Score: 0.0007)\n",
      "hillary (Score: 0.0006)\n",
      "israel (Score: 0.0005)\n",
      "election (Score: 0.0005)\n",
      "email (Score: 0.0004)\n",
      "u (Score: 0.0004)\n",
      "american (Score: 0.0004)\n",
      "state (Score: 0.0004)\n",
      "people (Score: 0.0004)\n",
      "world (Score: 0.0004)\n",
      "new (Score: 0.0003)\n",
      "obama (Score: 0.0003)\n",
      "russia (Score: 0.0003)\n",
      "donald (Score: 0.0003)\n",
      "would (Score: 0.0003)\n",
      "david (Score: 0.0003)\n",
      "video (Score: 0.0003)\n",
      "president (Score: 0.0003)\n",
      "war (Score: 0.0003)\n",
      "said (Score: 0.0003)\n",
      "gun (Score: 0.0003)\n",
      "one (Score: 0.0003)\n",
      "government (Score: 0.0003)\n",
      "iran (Score: 0.0003)\n",
      "medium (Score: 0.0003)\n",
      "vote (Score: 0.0003)\n",
      "white (Score: 0.0003)\n",
      "study (Score: 0.0003)\n",
      "fat (Score: 0.0003)\n",
      "\n",
      "Topic 3:\n",
      "clinton (Score: 0.0012)\n",
      "fbi (Score: 0.0007)\n",
      "email (Score: 0.0007)\n",
      "trump (Score: 0.0007)\n",
      "hillary (Score: 0.0007)\n",
      "election (Score: 0.0005)\n",
      "comey (Score: 0.0005)\n",
      "campaign (Score: 0.0004)\n",
      "podesta (Score: 0.0004)\n",
      "american (Score: 0.0004)\n",
      "said (Score: 0.0003)\n",
      "investigation (Score: 0.0003)\n",
      "aleppo (Score: 0.0003)\n",
      "time (Score: 0.0003)\n",
      "u (Score: 0.0003)\n",
      "president (Score: 0.0003)\n",
      "state (Score: 0.0003)\n",
      "day (Score: 0.0003)\n",
      "would (Score: 0.0003)\n",
      "law (Score: 0.0003)\n",
      "presidential (Score: 0.0003)\n",
      "new (Score: 0.0003)\n",
      "government (Score: 0.0003)\n",
      "information (Score: 0.0003)\n",
      "russian (Score: 0.0003)\n",
      "report (Score: 0.0003)\n",
      "bill (Score: 0.0003)\n",
      "wikileaks (Score: 0.0003)\n",
      "2016 (Score: 0.0003)\n",
      "source (Score: 0.0003)\n",
      "\n",
      "Topic 4:\n",
      "de (Score: 0.0003)\n",
      "galacticconnectioncom (Score: 0.0002)\n",
      "blowtorch (Score: 0.0002)\n",
      "trump (Score: 0.0002)\n",
      "la (Score: 0.0002)\n",
      "el (Score: 0.0002)\n",
      "dakota (Score: 0.0002)\n",
      "deport (Score: 0.0002)\n",
      "vitamin (Score: 0.0002)\n",
      "email (Score: 0.0002)\n",
      "pipeline (Score: 0.0002)\n",
      "en (Score: 0.0002)\n",
      "clinton (Score: 0.0002)\n",
      "hillary (Score: 0.0002)\n",
      "russia (Score: 0.0002)\n",
      "state (Score: 0.0001)\n",
      "que (Score: 0.0001)\n",
      "otis (Score: 0.0001)\n",
      "u (Score: 0.0001)\n",
      "postillon (Score: 0.0001)\n",
      "underwear (Score: 0.0001)\n",
      "american (Score: 0.0001)\n",
      "election (Score: 0.0001)\n",
      "news (Score: 0.0001)\n",
      "click (Score: 0.0001)\n",
      "der (Score: 0.0001)\n",
      "hayden (Score: 0.0001)\n",
      "cured (Score: 0.0001)\n",
      "turbeville (Score: 0.0001)\n",
      "people (Score: 0.0001)\n",
      "\n",
      "Topic 5:\n",
      "в (Score: 0.0009)\n",
      "и (Score: 0.0006)\n",
      "de (Score: 0.0005)\n",
      "la (Score: 0.0004)\n",
      "на (Score: 0.0004)\n",
      "trump (Score: 0.0004)\n",
      "clinton (Score: 0.0004)\n",
      "что (Score: 0.0003)\n",
      "el (Score: 0.0003)\n",
      "que (Score: 0.0003)\n",
      "не (Score: 0.0003)\n",
      "text (Score: 0.0003)\n",
      "с (Score: 0.0003)\n",
      "en (Score: 0.0003)\n",
      "russia (Score: 0.0003)\n",
      "palestinian (Score: 0.0002)\n",
      "hillary (Score: 0.0002)\n",
      "comment (Score: 0.0002)\n",
      "dotcom (Score: 0.0002)\n",
      "ha (Score: 0.0002)\n",
      "по (Score: 0.0002)\n",
      "как (Score: 0.0002)\n",
      "email (Score: 0.0002)\n",
      "isi (Score: 0.0002)\n",
      "u (Score: 0.0002)\n",
      "del (Score: 0.0002)\n",
      "это (Score: 0.0002)\n",
      "result (Score: 0.0002)\n",
      "evangelical (Score: 0.0002)\n",
      "israeli (Score: 0.0002)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Step 1: Identify the main topics in the dataset\n",
    "topics = lda_model.show_topics(num_topics = num_topics_selected, num_words = 30, formatted=False)\n",
    "\n",
    "# Step 2: Print the most representative words for each topic\n",
    "for topic_id, word_scores in topics:\n",
    "    print(f\"\\nTopic {topic_id + 1}:\")\n",
    "    for word, score in word_scores:\n",
    "        print(f\"{word} (Score: {score:.4f})\")\n",
    "        \n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce4c80",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf2f620e1116755891b494ba7d0af986",
     "grade": true,
     "grade_id": "cell-f5a6c428bb4b0f8e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7f77f",
   "metadata": {},
   "source": [
    "## Question 5: Improving Preprocessing for Topic Modeling (1 point)\n",
    "\n",
    "### Objective:\n",
    "Enhance your understanding and skills in preprocessing text data for topic modeling. You will focus on two key areas: \n",
    "1. Subsetting posts by language (focusing on English).\n",
    "2. Enriching the list of stopwords specific to your dataset for more effective topic modeling by adding custom stopwords. Analyze the results to identify irrelevant or overly common words that could be added to your stopwords list.\n",
    "3. **Re-run Topic Modeling**: Apply the enriched stopwords list and re-run the topic modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f98ce57",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f5d5247d657b77caa306959d724d0d4",
     "grade": true,
     "grade_id": "cell-20f891863be76484",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# subset dataset by english articles\n",
    "# news_df = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "custom_stopwords = set([])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b778b45",
   "metadata": {},
   "source": [
    "Does this additional preprocessing improve the topic model output? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eded2c1-0596-4228-b17d-e5cadc38b6d8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9650f3385095a74237c5ca125f6d4af0",
     "grade": true,
     "grade_id": "cell-a31aca76ea2afbe0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc00be",
   "metadata": {},
   "source": [
    "## Question 6. Assessing LDA Model Coherence (2 points)\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this exercise, you will assess the coherence of an LDA topic model using Gensim's coherence measures. Coherence measures help in evaluating how well the topics generated by the model are interpretable and semantically meaningful.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. **Implement an LDA Model**: Using the \"Fake news\" dataset, implement an LDA model as done in the previous exercises.\n",
    "2. **Compute Coherence Score**: Calculate the coherence score of your model using Gensim's CoherenceModel (https://radimrehurek.com/gensim/models/coherencemodel.html).\n",
    "3. **Experiment with Different Number of Topics**: Experiment with different numbers of topics (e.g., 5, 10, 15 or 10, 50, 100 or whatever range you deem likely for the given data) and assess how the coherence score changes. Write a function that computes a coherence score for each model and plot the coherence scores associated with each topic number value (1 point).\n",
    "4. **Interpret Results**: Based on the coherence scores, determine the optimal number of topics for the model (1 point).\n",
    "\n",
    "### Assessment Criteria\n",
    "\n",
    "- Quality of LDA model implementation.\n",
    "- Correct calculation and interpretation of coherence scores.\n",
    "- Thoughtful experimentation with different numbers of topics and analysis of the impact on coherence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521724f0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe4387f4926e8561eef9746a11c5ab03",
     "grade": true,
     "grade_id": "cell-29b1683e1a1c0550",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Function to compute coherence score\n",
    "def compute_coherence(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Applying the function to our dataset\n",
    "model_list, coherence_values = compute_coherence(dictionary=dictionary, corpus=doc_term_matrix, texts=news_df['cleaned_text'].str.split(), start=20, limit=100, step=10)\n",
    "\n",
    "# Plotting coherence scores\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93d418-8821-4917-869d-0ed54bb8586a",
   "metadata": {},
   "source": [
    "What is the optimal number of topics for your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b109b45-93ed-4379-9407-137c358929e6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a09e4fc2626ba2cebedfd573b8a4bb29",
     "grade": true,
     "grade_id": "cell-9237751dd218cf6e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c14865",
   "metadata": {},
   "source": [
    "## Question 7: Fitting the Final LDA Model on the Entire Dataset (4 points)\n",
    "\n",
    "### Objective:\n",
    "Having identified the optimal number of topics using the coherence model in Gensim, your task now is to apply this knowledge to fit the final LDA (Latent Dirichlet Allocation) model on the entire dataset.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. **Optimal Number of Topics**:\n",
    "   - Recall the optimal number of topics you determined using the coherence model on a sample of your dataset.\n",
    "   \n",
    "2. **Preprocess the Full Dataset**:\n",
    "   - Ensure that the entire dataset is properly preprocessed (tokenization, removing stopwords, etc.).\n",
    "   - Create a dictionary and a bag-of-words corpus using the full dataset.\n",
    "\n",
    "3. **Fit the LDA Model**:\n",
    "   - Instantiate and train the LDA model on the entire dataset using the optimal number of topics you previously determined.\n",
    "   - Use the same model parameters that were most effective during your experimentation with the sample.\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - Briefly evaluate the model by examining the coherence score on the full dataset.\n",
    "   - Display the top words for each topic and provide a brief interpretation.\n",
    "\n",
    "5. **Reflection**:\n",
    "   - Reflect on any differences observed in topic quality and coherence when the model is applied to the entire dataset versus the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673e197",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f60e39836c029ea56c19fdaedb2c2d32",
     "grade": true,
     "grade_id": "cell-93e0c36b6e485390",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
